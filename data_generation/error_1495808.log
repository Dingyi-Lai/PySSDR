
The following have been reloaded with a version change:
  1) devel/cuda/12.2 => devel/cuda/12.4

2025-01-30 01:29:50.685910: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-30 01:29:50.698746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738196990.710918 3444950 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738196990.714571 3444950 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-30 01:29:50.728898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:root:Starting parallel dataset generation...
2025-01-30 01:29:53.340125: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-30 01:29:53.340190: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: haicn1705.localdomain
2025-01-30 01:29:53.340205: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: haicn1705.localdomain
2025-01-30 01:29:53.340319: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: 550.54.15
2025-01-30 01:29:53.340349: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 550.54.15
2025-01-30 01:29:53.340357: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:259] kernel version seems to match DSO: 550.54.15
INFO:root:Generating dataset: Number of obs 10 | Replication 0
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib64/python3.9/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib64/python3.9/multiprocessing/pool.py", line 51, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File "/hkfs/home/haicore/scc/pa6512/PySSDR/data_generation/data_generation.py", line 186, in generate_task
    unstructured_effects, U_k, psi_k, b_k = generate_unstructured_effects(flattened_images, dnn_model, K)
  File "/hkfs/home/haicore/scc/pa6512/PySSDR/data_generation/data_generation.py", line 93, in generate_unstructured_effects
    penultimate_layer_model = Model(inputs=dnn_model.input, outputs=dnn_model.layers[-2].output)
  File "/software/all/jupyter/ai/2025-01-14/lib/python3.9/site-packages/keras/src/ops/operation.py", line 254, in input
    return self._get_node_attribute_at_index(0, "input_tensors", "input")
  File "/software/all/jupyter/ai/2025-01-14/lib/python3.9/site-packages/keras/src/ops/operation.py", line 285, in _get_node_attribute_at_index
    raise AttributeError(
AttributeError: The layer sequential has never been called and thus has no defined input.
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hkfs/home/haicore/scc/pa6512/PySSDR/data_generation/data_generation.py", line 274, in <module>
    scenarios_generate(n_list, distribution_list, SNR_list, grid_size, alpha_l, beta_nl, n_rep=1, n_cores=n_core,
  File "/hkfs/home/haicore/scc/pa6512/PySSDR/data_generation/data_generation.py", line 227, in scenarios_generate
    pool.starmap(generate_task, [(i, distribution_list, SNR_list, grid_size,
  File "/usr/lib64/python3.9/multiprocessing/pool.py", line 372, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "/usr/lib64/python3.9/multiprocessing/pool.py", line 771, in get
    raise self._value
  File "/usr/lib64/python3.9/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib64/python3.9/multiprocessing/pool.py", line 51, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File "/hkfs/home/haicore/scc/pa6512/PySSDR/data_generation/data_generation.py", line 186, in generate_task
    unstructured_effects, U_k, psi_k, b_k = generate_unstructured_effects(flattened_images, dnn_model, K)
  File "/hkfs/home/haicore/scc/pa6512/PySSDR/data_generation/data_generation.py", line 93, in generate_unstructured_effects
    penultimate_layer_model = Model(inputs=dnn_model.input, outputs=dnn_model.layers[-2].output)
  File "/software/all/jupyter/ai/2025-01-14/lib/python3.9/site-packages/keras/src/ops/operation.py", line 254, in input
    return self._get_node_attribute_at_index(0, "input_tensors", "input")
  File "/software/all/jupyter/ai/2025-01-14/lib/python3.9/site-packages/keras/src/ops/operation.py", line 285, in _get_node_attribute_at_index
    raise AttributeError(
AttributeError: The layer sequential has never been called and thus has no defined input.
cp: cannot stat '/scratch/slurm_tmpdir/job_1495808/output': No such file or directory
